{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b934b8b",
   "metadata": {},
   "source": [
    "#### **Recurrent Network Testing Notebook**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c1054b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "import string\n",
    "import random\n",
    "\n",
    "import os\n",
    "from PIL import Image\n",
    "import cv2\n",
    "\n",
    "import torch\n",
    "from torch import optim\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, Subset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchvision import transforms\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a0b4fe",
   "metadata": {},
   "source": [
    "#### **Training Dataset import**\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94224b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Chars74kDataset(Dataset):\n",
    "    def __init__(self, root_dir, gen_transforms=None, train_transforms=None, train=True):\n",
    "\n",
    "        self.root_dir = root_dir\n",
    "        self.gen_transforms = gen_transforms\n",
    "        self.train_transforms = train_transforms\n",
    "        self.train = train\n",
    "\n",
    "        self.samples = []\n",
    "        for cls in sorted(os.listdir(root_dir)):\n",
    "            cls_dir = os.path.join(root_dir, cls)\n",
    "            if not os.path.isdir(cls_dir):\n",
    "                continue\n",
    "            for fname in os.listdir(cls_dir):\n",
    "                if fname.lower().endswith('.png'):\n",
    "                    self.samples.append((os.path.join(cls_dir, fname), cls))\n",
    "\n",
    "        self.classes = sorted({label for img, label in self.samples})\n",
    "        self.cls2idx = {cls: idx + 1 for idx, cls in enumerate(self.classes)}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path, cls = self.samples[idx]\n",
    "        img = Image.open(path).convert(\"L\")\n",
    "\n",
    "        if self.gen_transforms:\n",
    "            img = self.gen_transforms(img)\n",
    "        if self.train and self.train_transforms:\n",
    "            img = self.train_transforms(img)\n",
    "\n",
    "        label = self.cls2idx[cls]\n",
    "        return img, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "31cb2964",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_transform = transforms.Compose([\n",
    "    transforms.Resize((28, 28)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Lambda(lambda x: (x < 0.7).float())\n",
    "])\n",
    "\n",
    "train_augment = transforms.Compose([\n",
    "    transforms.RandomVerticalFlip(0.2),\n",
    "    transforms.RandomRotation(degrees=10),\n",
    "    transforms.RandomAffine(\n",
    "        degrees=0,\n",
    "        translate=(0.1, 0.1),\n",
    "        scale=(0.9, 1.1)\n",
    "    ),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4fb0059",
   "metadata": {},
   "source": [
    "#### **Séparation des données**\n",
    "\n",
    "Pour garantir une évaluation fiable et éviter toute fuite d’information (data leakage) :\n",
    "\n",
    "* **3 ensembles distincts** :\n",
    "\n",
    "  * `train` (65 %) : apprentissage des paramètres du modèle.\n",
    "  * `val` (15 %) : réglage des hyperparamètres et choix de la meilleure version du modèle.\n",
    "  * `test` (20 %) : mesure finale de performance, à n’utiliser qu’une seule fois après la phase de validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "43740c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_dataset = Chars74kDataset(root_dir='C:/Users/G-PROGNOS-01/Desktop/Slomiany Baptiste/doc_lisibility/data/Chars74k/EnglishImg', gen_transforms=gen_transform,\n",
    "                               train_transforms=None, train=False)\n",
    "all_labels = np.array([full_dataset[i][1] for i in range(len(full_dataset))])\n",
    "\n",
    "sss1 = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "temp_idx, test_idx = next(sss1.split(np.zeros(len(all_labels)), all_labels))\n",
    "\n",
    "temp_labels = all_labels[temp_idx]\n",
    "val_size = 0.15 / 0.8\n",
    "sss2 = StratifiedShuffleSplit(n_splits=1, test_size=val_size, random_state=42)\n",
    "train_subidx, val_subidx = next(sss2.split(np.zeros(len(temp_labels)), temp_labels))\n",
    "\n",
    "train_idx = temp_idx[train_subidx]\n",
    "val_idx   = temp_idx[val_subidx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7607e915",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Chars74kDataset(root_dir='C:/Users/G-PROGNOS-01/Desktop/Slomiany Baptiste/doc_lisibility/data/Chars74k/EnglishImg', gen_transforms=gen_transform,\n",
    "                                train_transforms=train_augment, train=True)\n",
    "train_ds = Subset(train_dataset, train_idx)\n",
    "\n",
    "val_dataset = Chars74kDataset(root_dir='C:/Users/G-PROGNOS-01/Desktop/Slomiany Baptiste/doc_lisibility/data/Chars74k/EnglishImg', gen_transforms=gen_transform,\n",
    "                              train_transforms=None, train=False)\n",
    "val_ds = Subset(val_dataset, val_idx)\n",
    "\n",
    "test_dataset = Chars74kDataset(root_dir='C:/Users/G-PROGNOS-01/Desktop/Slomiany Baptiste/doc_lisibility/data/Chars74k/EnglishImg', gen_transforms=gen_transform,\n",
    "                               train_transforms=None, train=False)\n",
    "test_ds = Subset(test_dataset, test_idx)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=128, shuffle=True)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=128, shuffle=False)\n",
    "test_loader  = DataLoader(test_ds,  batch_size=128, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "83208b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "characters = list(string.digits + string.ascii_uppercase + string.ascii_lowercase)\n",
    "class_names = {i + 1: char for i, char in enumerate(characters)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9023c39e",
   "metadata": {},
   "source": [
    "#### **Model definition and Training**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dd41ccdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Train function\n",
    "def train_model( model, train_loader, val_loader, num_epochs,\n",
    "                 criterion, optimizer, writer ):\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        running_loss, running_corrects = 0.0, 0\n",
    "\n",
    "        for inputs, labels in train_loader:\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            result = model(inputs)\n",
    "            outputs = result\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            preds = outputs.argmax(dim=1)\n",
    "            running_corrects += (preds == labels).sum().item()\n",
    "\n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "        epoch_acc = running_corrects / len(train_loader.dataset)\n",
    "\n",
    "        model.eval()\n",
    "        val_loss, val_corrects = 0.0, 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                result = model(inputs)\n",
    "                outputs = result\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item() * inputs.size(0)\n",
    "                preds = outputs.argmax(dim=1)\n",
    "                val_corrects += (preds == labels).sum().item()\n",
    "\n",
    "        val_loss = val_loss / len(val_loader.dataset)\n",
    "        val_acc = val_corrects / len(val_loader.dataset)\n",
    "\n",
    "        if epoch % 1 == 0:\n",
    "            print(f\"Epoch {epoch}/{num_epochs} | \"\n",
    "                  f\"Train Loss: {epoch_loss:.4f}, Train Acc: {epoch_acc:.4f} | \"\n",
    "                  f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "        writer.add_scalar(\"Loss/Train\", epoch_loss, epoch)\n",
    "        writer.add_scalar(\"Loss/Val\", val_loss, epoch)\n",
    "        writer.add_scalar(\"Acc/Train\", epoch_acc, epoch)\n",
    "        writer.add_scalar(\"Acc/Val\", val_acc, epoch)\n",
    "\n",
    "        for name, param in model.named_parameters():\n",
    "            writer.add_histogram(name, param.cpu(), epoch)\n",
    "            if param.grad is not None:\n",
    "                writer.add_histogram(f\"{name}_grad\", param.grad.cpu(), epoch)\n",
    "\n",
    "    writer.close()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa41160",
   "metadata": {},
   "source": [
    "#### **SimpleRNN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aaf6dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, cls_dim):\n",
    "        super().__init__()\n",
    "        self.rnn = nn.RNN(input_dim, hidden_dim, 2, bidirectional=True, batch_first=True)\n",
    "        self.out = nn.Linear(hidden_dim * 4, cls_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output_sequence, final_hidden_state = self.rnn(x.squeeze(1))\n",
    "        y = final_hidden_state.permute(1, 0, 2).flatten(start_dim=1)\n",
    "\n",
    "        return self.out(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d56c457",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 | Train Loss: 3.7573, Train Acc: 0.0928 | Val Loss: 3.0211, Val Acc: 0.2017\n",
      "Epoch 2/5 | Train Loss: 3.4831, Train Acc: 0.1523 | Val Loss: 2.5885, Val Acc: 0.3148\n",
      "Epoch 3/5 | Train Loss: 2.9216, Train Acc: 0.2529 | Val Loss: 2.2016, Val Acc: 0.3977\n",
      "Epoch 4/5 | Train Loss: 3.1068, Train Acc: 0.2250 | Val Loss: 2.4828, Val Acc: 0.3048\n",
      "Epoch 5/5 | Train Loss: 3.2655, Train Acc: 0.1950 | Val Loss: 2.7834, Val Acc: 0.2487\n"
     ]
    }
   ],
   "source": [
    "### Parametrisation\n",
    "\n",
    "model = RNNClassifier(28, 128, 63)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-2, weight_decay=1e-2)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "writer = SummaryWriter(\"runs/rnn_squencerTrain0\")\n",
    "epochs = 5\n",
    "\n",
    "model = train_model(model, train_loader, val_loader, epochs, criterion, optimizer, writer)\n",
    "torch.save(model.state_dict(), \"rnn_model0.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f21821a4",
   "metadata": {},
   "source": [
    "#### **GRUModel**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "181c5235",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, cls_dim):\n",
    "        super().__init__()\n",
    "        self.gru = nn.GRU(input_dim, hidden_dim, 2, bidirectional=True, batch_first=True)\n",
    "        self.out = nn.Linear(hidden_dim * 4, cls_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output_sequence, final_hidden_state = self.gru(x.squeeze(1))\n",
    "        y = final_hidden_state.permute(1, 0, 2).flatten(start_dim=1)\n",
    "\n",
    "        return self.out(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d387940",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 | Train Loss: 1.7451, Train Acc: 0.5126 | Val Loss: 0.8008, Val Acc: 0.7635\n",
      "Epoch 2/5 | Train Loss: 0.9568, Train Acc: 0.7082 | Val Loss: 0.6637, Val Acc: 0.7893\n",
      "Epoch 3/5 | Train Loss: 0.8388, Train Acc: 0.7323 | Val Loss: 0.6366, Val Acc: 0.8039\n",
      "Epoch 4/5 | Train Loss: 0.7780, Train Acc: 0.7498 | Val Loss: 0.6028, Val Acc: 0.8034\n",
      "Epoch 5/5 | Train Loss: 0.7462, Train Acc: 0.7571 | Val Loss: 0.6089, Val Acc: 0.7997\n"
     ]
    }
   ],
   "source": [
    "### Parametrisation\n",
    "\n",
    "model = GRUClassifier(28, 128, 63)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-2, weight_decay=1e-2)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "writer = SummaryWriter(\"runs/gru_squencerTrain0\")\n",
    "epochs = 5\n",
    "\n",
    "model = train_model(model, train_loader, val_loader, epochs, criterion, optimizer, writer)\n",
    "torch.save(model.state_dict(), \"gru_model0.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d37f19b",
   "metadata": {},
   "source": [
    "#### **LSTM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "861420f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, cls_dim):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers=1, \n",
    "                            bidirectional=True, batch_first=True)\n",
    "\n",
    "        self.out = nn.Linear(hidden_dim * 2, cls_dim)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        output_sequence, (h_n, c_n) = self.lstm(x.squeeze(1))\n",
    "        final_layer_h_n = h_n[-2:, :, :]\n",
    "            \n",
    "        y = final_layer_h_n.permute(1, 0, 2).flatten(start_dim=1)\n",
    "        return self.out(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f3caca9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 | Train Loss: 2.2263, Train Acc: 0.3898 | Val Loss: 1.0229, Val Acc: 0.7010\n",
      "Epoch 2/5 | Train Loss: 1.1644, Train Acc: 0.6471 | Val Loss: 0.7784, Val Acc: 0.7544\n",
      "Epoch 3/5 | Train Loss: 0.9424, Train Acc: 0.7081 | Val Loss: 0.6541, Val Acc: 0.7952\n",
      "Epoch 4/5 | Train Loss: 0.8399, Train Acc: 0.7324 | Val Loss: 0.6336, Val Acc: 0.7950\n",
      "Epoch 5/5 | Train Loss: 0.7734, Train Acc: 0.7503 | Val Loss: 0.5804, Val Acc: 0.8120\n"
     ]
    }
   ],
   "source": [
    "### Parametrisation\n",
    "\n",
    "model = LSTMClassifier(28, 128, 63)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-2, weight_decay=1e-2)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "writer = SummaryWriter(\"runs/lstm_squencerTrain0\")\n",
    "epochs = 5\n",
    "\n",
    "model = train_model(model, train_loader, val_loader, epochs, criterion, optimizer, writer)\n",
    "torch.save(model.state_dict(), \"lstm_model0.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb451359",
   "metadata": {},
   "source": [
    "#### **LSTM + Attention**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1a899a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTMAttnClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, cls_dim):\n",
    "        super().__init__()\n",
    "        self.lstm_cell = nn.LSTM(input_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
    "        \n",
    "        self.attn = nn.Parameter(torch.randn(hidden_dim * 2))\n",
    "        self._out = nn.Linear(hidden_dim * 2, cls_dim)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        outputs, (hn, c_n) = self.lstm_cell(inputs.squeeze(1))\n",
    "\n",
    "        attn_scores = torch.matmul(outputs, self.attn)\n",
    "        attn_weights = torch.softmax(attn_scores, dim=1).unsqueeze(-1)\n",
    "\n",
    "        context = torch.sum(attn_weights * outputs, dim = 1)\n",
    "\n",
    "        logits = self._out(context)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd987d43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 | Train Loss: 2.1074, Train Acc: 0.4292 | Val Loss: 1.0081, Val Acc: 0.7093\n",
      "Epoch 2/5 | Train Loss: 1.1536, Train Acc: 0.6595 | Val Loss: 0.7764, Val Acc: 0.7567\n",
      "Epoch 3/5 | Train Loss: 0.9630, Train Acc: 0.7092 | Val Loss: 0.6840, Val Acc: 0.7845\n",
      "Epoch 4/5 | Train Loss: 0.8561, Train Acc: 0.7352 | Val Loss: 0.6153, Val Acc: 0.8051\n",
      "Epoch 5/5 | Train Loss: 0.8020, Train Acc: 0.7484 | Val Loss: 0.5863, Val Acc: 0.8080\n"
     ]
    }
   ],
   "source": [
    "### Parametrisation\n",
    "\n",
    "model = BiLSTMAttnClassifier(28, 128, 63)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-2, weight_decay=1e-2)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "writer = SummaryWriter(\"runs/lstmAttn_squencerTrain0\")\n",
    "epochs = 5\n",
    "\n",
    "model = train_model(model, train_loader, val_loader, epochs, criterion, optimizer, writer)\n",
    "torch.save(model.state_dict(), \"lstmAttn_model0.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a01de058",
   "metadata": {},
   "source": [
    "#### **HybridCNNRecurrent**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f0ee64c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGGBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels//4, 3, 1, 1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels//4),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels//4, out_channels, 3, 1, 1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.block(x)\n",
    "    \n",
    "class HybridNet(nn.Module):\n",
    "    def __init__(self, hidden_dim, cls_dim):\n",
    "        super().__init__()\n",
    "        self.bloc1 = VGGBlock(1, 16)\n",
    "        self.bloc2 = VGGBlock(16, 64)\n",
    "\n",
    "        self.lstm = nn.LSTM(64, hidden_dim, 1, batch_first=True, bidirectional=True)\n",
    "\n",
    "        self.attn = nn.Parameter(torch.randn(hidden_dim*2))\n",
    "        self._out = nn.Linear(hidden_dim * 2, cls_dim)\n",
    "\n",
    "    def forward(self, x): # input = (batch, 1, 28, 28)\n",
    "        x = self.bloc2(self.bloc1(x)) # (batch_size, channels = 64, height = 7, width = 7)\n",
    "        x = torch.flatten(x, 2).permute(0, 2, 1) # (batch, 49 = seq_length, channels)\n",
    "        outputs, (h_n, c_n) = self.lstm(x)\n",
    "\n",
    "        attn_scores = torch.matmul(outputs, self.attn)\n",
    "        attn_weights = torch.softmax(attn_scores, dim=1).unsqueeze(-1)\n",
    "\n",
    "        context = torch.sum(attn_weights * outputs, dim = 1)\n",
    "\n",
    "        logits = self._out(context)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce5f790",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 | Train Loss: 2.5633, Train Acc: 0.2871 | Val Loss: 1.4279, Val Acc: 0.5615\n",
      "Epoch 2/5 | Train Loss: 1.2875, Train Acc: 0.6070 | Val Loss: 0.8332, Val Acc: 0.7440\n",
      "Epoch 3/5 | Train Loss: 0.9488, Train Acc: 0.7005 | Val Loss: 0.7341, Val Acc: 0.7565\n",
      "Epoch 4/5 | Train Loss: 0.8241, Train Acc: 0.7337 | Val Loss: 0.6359, Val Acc: 0.7934\n",
      "Epoch 5/5 | Train Loss: 0.7565, Train Acc: 0.7493 | Val Loss: 0.5801, Val Acc: 0.8063\n"
     ]
    }
   ],
   "source": [
    "### Parametrisation\n",
    "\n",
    "model = HybridNet(128, 63)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-2, weight_decay=1e-2)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "writer = SummaryWriter(\"runs/HybridNetTrain0\")\n",
    "epochs = 5\n",
    "\n",
    "model = train_model(model, train_loader, val_loader, epochs, criterion, optimizer, writer)\n",
    "torch.save(model.state_dict(), \"HybridNet0.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6121f193",
   "metadata": {},
   "source": [
    "#### **VisionEncoder**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8445c80a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleViT(nn.Module):\n",
    "    def __init__(self, img_size=28, patch_size=4, num_classes=63, dim=8, depth=2, heads=2):\n",
    "        super().__init__()\n",
    "\n",
    "        num_patches = (img_size // patch_size) ** 2\n",
    "        patch_dim = patch_size * patch_size\n",
    "\n",
    "        self.patch_embed = nn.Linear(patch_dim, dim)\n",
    "\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, num_patches + 1, dim))\n",
    "        self.att_token = nn.Parameter(torch.randn(1, 1, dim))\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=dim, nhead=heads, dim_feedforward=64, dropout=0.1, activation='gelu', batch_first=True)\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=depth)\n",
    "\n",
    "        self.mlp_head = nn.Sequential(\n",
    "            nn.LayerNorm(dim),\n",
    "            nn.Linear(dim, num_classes)\n",
    "        )\n",
    "\n",
    "        self.patch_size = patch_size\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch, canal, height, width = x.shape\n",
    "        p = self.patch_size\n",
    "\n",
    "        x = x.unfold(2, p, p).unfold(3, p, p)\n",
    "        x = x.contiguous().view(batch, canal, -1, p, p)\n",
    "        x = x.permute(0, 2, 1, 3, 4).flatten(2)\n",
    "\n",
    "        x = self.patch_embed(x)\n",
    "\n",
    "        att_tokens = self.att_token.expand(batch, -1, -1)\n",
    "        x = torch.cat((att_tokens, x), dim=1)\n",
    "\n",
    "        x = x + self.pos_embedding[:, :x.size(1), :]\n",
    "\n",
    "        x = self.transformer(x)\n",
    "\n",
    "        cls_output = x[:, 0]\n",
    "        out = self.mlp_head(cls_output)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b08f0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 | Train Loss: 3.8410, Train Acc: 0.0438 | Val Loss: 3.3424, Val Acc: 0.1115\n",
      "Epoch 2/5 | Train Loss: 3.3893, Train Acc: 0.1043 | Val Loss: 2.9069, Val Acc: 0.1882\n",
      "Epoch 3/5 | Train Loss: 3.1241, Train Acc: 0.1508 | Val Loss: 2.5723, Val Acc: 0.2679\n",
      "Epoch 4/5 | Train Loss: 2.9850, Train Acc: 0.1744 | Val Loss: 2.5649, Val Acc: 0.2636\n",
      "Epoch 5/5 | Train Loss: 2.9212, Train Acc: 0.1866 | Val Loss: 2.3329, Val Acc: 0.3298\n"
     ]
    }
   ],
   "source": [
    "### Parametrisation\n",
    "\n",
    "transformer = SimpleViT(63)\n",
    "optimizer = optim.AdamW(transformer.parameters(), lr=1e-2, weight_decay=1e-2)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "writer = SummaryWriter(\"runs/transformer_fromscratch_train0_\")\n",
    "epochs = 5\n",
    "\n",
    "transformer = train_model(transformer, train_loader, val_loader, epochs, criterion, optimizer, writer)\n",
    "torch.save(transformer.state_dict(), \"transformer0.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
