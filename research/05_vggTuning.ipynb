{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b934b8b",
   "metadata": {},
   "source": [
    "#### **Model from Scratch**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c1054b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "import string\n",
    "import random\n",
    "\n",
    "import os\n",
    "from PIL import Image\n",
    "import cv2\n",
    "\n",
    "import torch\n",
    "from torch import optim\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, Subset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchvision import transforms\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a0b4fe",
   "metadata": {},
   "source": [
    "#### **Training Dataset import**\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94224b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Chars74kDataset(Dataset):\n",
    "    def __init__(self, root_dir, gen_transforms=None, train_transforms=None, train=True):\n",
    "\n",
    "        self.root_dir = root_dir\n",
    "        self.gen_transforms = gen_transforms\n",
    "        self.train_transforms = train_transforms\n",
    "        self.train = train\n",
    "\n",
    "        self.samples = []\n",
    "        for cls in sorted(os.listdir(root_dir)):\n",
    "            cls_dir = os.path.join(root_dir, cls)\n",
    "            if not os.path.isdir(cls_dir):\n",
    "                continue\n",
    "            for fname in os.listdir(cls_dir):\n",
    "                if fname.lower().endswith('.png'):\n",
    "                    self.samples.append((os.path.join(cls_dir, fname), cls))\n",
    "\n",
    "        self.classes = sorted({label for img, label in self.samples})\n",
    "        self.cls2idx = {cls: idx + 1 for idx, cls in enumerate(self.classes)}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path, cls = self.samples[idx]\n",
    "        img = Image.open(path).convert(\"L\")\n",
    "\n",
    "        if self.gen_transforms:\n",
    "            img = self.gen_transforms(img)\n",
    "        if self.train and self.train_transforms:\n",
    "            img = self.train_transforms(img)\n",
    "\n",
    "        label = self.cls2idx[cls]\n",
    "        return img, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "31cb2964",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_transform = transforms.Compose([\n",
    "    transforms.Resize((28, 28)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Lambda(lambda x: (x < 0.7).float())\n",
    "])\n",
    "\n",
    "train_augment = transforms.Compose([\n",
    "    transforms.RandomVerticalFlip(0.2),\n",
    "    transforms.RandomRotation(degrees=10),\n",
    "    transforms.RandomAffine(\n",
    "        degrees=0,\n",
    "        translate=(0.1, 0.1),\n",
    "        scale=(0.9, 1.1)\n",
    "    ),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4fb0059",
   "metadata": {},
   "source": [
    "#### **Séparation des données**\n",
    "\n",
    "Pour garantir une évaluation fiable et éviter toute fuite d’information (data leakage) :\n",
    "\n",
    "* **3 ensembles distincts** :\n",
    "\n",
    "  * `train` (65 %) : apprentissage des paramètres du modèle.\n",
    "  * `val` (15 %) : réglage des hyperparamètres et choix de la meilleure version du modèle.\n",
    "  * `test` (20 %) : mesure finale de performance, à n’utiliser qu’une seule fois après la phase de validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "43740c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_dataset = Chars74kDataset(root_dir='C:/Users/G-PROGNOS-01/Desktop/Slomiany Baptiste/doc_lisibility/data/Chars74k/EnglishImg', gen_transforms=gen_transform,\n",
    "                               train_transforms=None, train=False)\n",
    "all_labels = np.array([full_dataset[i][1] for i in range(len(full_dataset))])\n",
    "\n",
    "sss1 = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "temp_idx, test_idx = next(sss1.split(np.zeros(len(all_labels)), all_labels))\n",
    "\n",
    "temp_labels = all_labels[temp_idx]\n",
    "val_size = 0.15 / 0.8\n",
    "sss2 = StratifiedShuffleSplit(n_splits=1, test_size=val_size, random_state=42)\n",
    "train_subidx, val_subidx = next(sss2.split(np.zeros(len(temp_labels)), temp_labels))\n",
    "\n",
    "train_idx = temp_idx[train_subidx]\n",
    "val_idx   = temp_idx[val_subidx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7607e915",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Chars74kDataset(root_dir='C:/Users/G-PROGNOS-01/Desktop/Slomiany Baptiste/doc_lisibility/data/Chars74k/EnglishImg', gen_transforms=gen_transform,\n",
    "                                train_transforms=train_augment, train=True)\n",
    "train_ds = Subset(train_dataset, train_idx)\n",
    "\n",
    "val_dataset = Chars74kDataset(root_dir='C:/Users/G-PROGNOS-01/Desktop/Slomiany Baptiste/doc_lisibility/data/Chars74k/EnglishImg', gen_transforms=gen_transform,\n",
    "                              train_transforms=None, train=False)\n",
    "val_ds = Subset(val_dataset, val_idx)\n",
    "\n",
    "test_dataset = Chars74kDataset(root_dir='C:/Users/G-PROGNOS-01/Desktop/Slomiany Baptiste/doc_lisibility/data/Chars74k/EnglishImg', gen_transforms=gen_transform,\n",
    "                               train_transforms=None, train=False)\n",
    "test_ds = Subset(test_dataset, test_idx)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=128, shuffle=True)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=128, shuffle=False)\n",
    "test_loader  = DataLoader(test_ds,  batch_size=128, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "83208b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "characters = list(string.digits + string.ascii_uppercase + string.ascii_lowercase)\n",
    "class_names = {i + 1: char for i, char in enumerate(characters)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9023c39e",
   "metadata": {},
   "source": [
    "#### **Model definition and Training**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5fd91532",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Train function\n",
    "def train_model( model, train_loader, val_loader, num_epochs,\n",
    "                 criterion, optimizer, writer ):\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        running_loss, running_corrects = 0.0, 0\n",
    "\n",
    "        for inputs, labels in train_loader:\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            result = model(inputs)\n",
    "            outputs = result[0] if isinstance(result, tuple) else result ## prise en compte retour activations\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            preds = outputs.argmax(dim=1)\n",
    "            running_corrects += (preds == labels).sum().item()\n",
    "\n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "        epoch_acc = running_corrects / len(train_loader.dataset)\n",
    "\n",
    "        model.eval()\n",
    "        val_loss, val_corrects = 0.0, 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                result = model(inputs)\n",
    "                outputs = result[0] if isinstance(result, tuple) else result\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item() * inputs.size(0)\n",
    "                preds = outputs.argmax(dim=1)\n",
    "                val_corrects += (preds == labels).sum().item()\n",
    "\n",
    "        val_loss = val_loss / len(val_loader.dataset)\n",
    "        val_acc = val_corrects / len(val_loader.dataset)\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"Epoch {epoch}/{num_epochs} | \"\n",
    "                  f\"Train Loss: {epoch_loss:.4f}, Train Acc: {epoch_acc:.4f} | \"\n",
    "                  f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "        writer.add_scalar(\"Loss/Train\", epoch_loss, epoch)\n",
    "        writer.add_scalar(\"Loss/Val\", val_loss, epoch)\n",
    "        writer.add_scalar(\"Acc/Train\", epoch_acc, epoch)\n",
    "        writer.add_scalar(\"Acc/Val\", val_acc, epoch)\n",
    "\n",
    "        for name, param in model.named_parameters():\n",
    "            writer.add_histogram(name, param.cpu(), epoch)\n",
    "            if param.grad is not None:\n",
    "                writer.add_histogram(f\"{name}_grad\", param.grad.cpu(), epoch)\n",
    "\n",
    "        images, _ = next(iter(val_loader))\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            result = model(images, log_activations=True)\n",
    "            outputs, activations = result if isinstance(result, tuple) else (result, {})\n",
    "        for layer, activation in activations.items():\n",
    "            if activation.ndim == 4:\n",
    "                for i in range(activation.size(1)):\n",
    "                    writer.add_image(\n",
    "                        f\"Activations/{layer}/{i}\",\n",
    "                        activation[0, i].unsqueeze(0).cpu(),\n",
    "                        epoch)\n",
    "\n",
    "    writer.close()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c21a2b4f",
   "metadata": {},
   "source": [
    "#### **1st Expansion -- Width**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ef2b2fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGGBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels//4, 3, 1, 1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels//4),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Conv2d(out_channels//4, out_channels, 3, 1, 1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.MaxPool2d(2, 2)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.block(x)\n",
    "    \n",
    "class VGGLikeNN1(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        self.block1 = VGGBlock(1, 16)\n",
    "        self.block2 = VGGBlock(16, 64)\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.LazyLinear(num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, log_activations=False):\n",
    "        activations = {}\n",
    "\n",
    "        x = self.block1(x)\n",
    "        if log_activations:\n",
    "            activations['block1_out'] = x\n",
    "\n",
    "        x = self.block2(x)\n",
    "        if log_activations:\n",
    "            activations['block2_out'] = x\n",
    "\n",
    "        logits = self.classifier(x)\n",
    "        if log_activations:\n",
    "            activations['logits'] = logits\n",
    "            return logits, activations\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "12b08f0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/30 | Train Loss: 0.6768, Train Acc: 0.7845 | Val Loss: 0.5069, Val Acc: 0.8260\n",
      "Epoch 20/30 | Train Loss: 0.5895, Train Acc: 0.8020 | Val Loss: 0.4590, Val Acc: 0.8438\n",
      "Epoch 30/30 | Train Loss: 0.5332, Train Acc: 0.8178 | Val Loss: 0.4051, Val Acc: 0.8588\n"
     ]
    }
   ],
   "source": [
    "### Parametrisation\n",
    "\n",
    "model1 = VGGLikeNN1(63)\n",
    "optimizer = optim.Adam(model1.parameters(), lr=5e-3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "writer = SummaryWriter(\"runsFineTune/vgg_training1\")\n",
    "epochs = 30\n",
    "\n",
    "model1 = train_model(model1, train_loader, val_loader, epochs, criterion, optimizer, writer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "887ee6e9",
   "metadata": {},
   "source": [
    "#### **Model 2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "84583995",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGGLikeNN(nn.Module):\n",
    "    def __init__(self, channels_param, num_classes):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.features = nn.ModuleList()\n",
    "        for in_chan, out_chan in channels_param:\n",
    "            self.features.append(VGGBlock(in_chan, out_chan))\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.LazyLinear(num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, log_activations=False):\n",
    "        activations = {}\n",
    "\n",
    "        for i, block in enumerate(self.features):\n",
    "            x = block(x)\n",
    "            if log_activations:\n",
    "                activations[f'block{i+1}_out'] = x\n",
    "\n",
    "        logits = self.classifier(x)\n",
    "        if log_activations:\n",
    "            activations['logits'] = logits\n",
    "            return logits, activations\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e911b7da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/30 | Train Loss: 0.6437, Train Acc: 0.7928 | Val Loss: 0.5073, Val Acc: 0.8341\n",
      "Epoch 20/30 | Train Loss: 0.5275, Train Acc: 0.8190 | Val Loss: 0.4182, Val Acc: 0.8541\n",
      "Epoch 30/30 | Train Loss: 0.4773, Train Acc: 0.8313 | Val Loss: 0.3858, Val Acc: 0.8606\n"
     ]
    }
   ],
   "source": [
    "### Parametrisation\n",
    "\n",
    "model1 = VGGLikeNN([(1, 32), (32, 128)], 63)\n",
    "optimizer = optim.Adam(model1.parameters(), lr=5e-3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "writer = SummaryWriter(\"runsFineTune/vgg_training2\")\n",
    "epochs = 30\n",
    "\n",
    "model1 = train_model(model1, train_loader, val_loader, epochs, criterion, optimizer, writer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8202c88f",
   "metadata": {},
   "source": [
    "#### **Model 3**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7d868f3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/30 | Train Loss: 0.5340, Train Acc: 0.8125 | Val Loss: 0.4346, Val Acc: 0.8427\n",
      "Epoch 20/30 | Train Loss: 0.4489, Train Acc: 0.8333 | Val Loss: 0.3813, Val Acc: 0.8568\n",
      "Epoch 30/30 | Train Loss: 0.4164, Train Acc: 0.8423 | Val Loss: 0.3679, Val Acc: 0.8622\n"
     ]
    }
   ],
   "source": [
    "### Parametrisation\n",
    "\n",
    "model = VGGLikeNN([(1, 16), (16, 64), (64, 128)], 63)\n",
    "optimizer = optim.Adam(model.parameters(), lr=5e-3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "writer = SummaryWriter(\"runsFineTune/vgg_training3\")\n",
    "epochs = 30\n",
    "\n",
    "model = train_model(model, train_loader, val_loader, epochs, criterion, optimizer, writer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b41211b3",
   "metadata": {},
   "source": [
    "#### **Model 4**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f83f780a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/30 | Train Loss: 0.4928, Train Acc: 0.8233 | Val Loss: 0.4136, Val Acc: 0.8515\n",
      "Epoch 20/30 | Train Loss: 0.4139, Train Acc: 0.8441 | Val Loss: 0.3499, Val Acc: 0.8695\n",
      "Epoch 30/30 | Train Loss: 0.3647, Train Acc: 0.8600 | Val Loss: 0.3485, Val Acc: 0.8633\n"
     ]
    }
   ],
   "source": [
    "### Parametrisation\n",
    "\n",
    "model = VGGLikeNN([(1, 32), (32, 128), (128, 256)], 63)\n",
    "optimizer = optim.Adam(model.parameters(), lr=5e-3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "writer = SummaryWriter(\"runsFineTune/vgg_training4\")\n",
    "epochs = 30\n",
    "\n",
    "model = train_model(model, train_loader, val_loader, epochs, criterion, optimizer, writer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e2478dc",
   "metadata": {},
   "source": [
    "#### **Model 5**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1f8eb6a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/30 | Train Loss: 0.4827, Train Acc: 0.8250 | Val Loss: 0.4017, Val Acc: 0.8522\n",
      "Epoch 20/30 | Train Loss: 0.3950, Train Acc: 0.8514 | Val Loss: 0.3779, Val Acc: 0.8617\n",
      "Epoch 30/30 | Train Loss: 0.3471, Train Acc: 0.8649 | Val Loss: 0.3288, Val Acc: 0.8771\n"
     ]
    }
   ],
   "source": [
    "### Parametrisation\n",
    "\n",
    "model = VGGLikeNN([(1, 32), (32, 256), (256, 1024)], 63)\n",
    "optimizer = optim.Adam(model.parameters(), lr=5e-3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "writer = SummaryWriter(\"runsFineTune/vgg_training5\")\n",
    "epochs = 30\n",
    "\n",
    "model = train_model(model, train_loader, val_loader, epochs, criterion, optimizer, writer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d426679",
   "metadata": {},
   "source": [
    "#### **Model 6**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "252fd1ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/30 | Train Loss: 0.4580, Train Acc: 0.8277 | Val Loss: 0.3927, Val Acc: 0.8489\n",
      "Epoch 20/30 | Train Loss: 0.3773, Train Acc: 0.8521 | Val Loss: 0.3416, Val Acc: 0.8677\n",
      "Epoch 30/30 | Train Loss: 0.3303, Train Acc: 0.8681 | Val Loss: 0.3173, Val Acc: 0.8801\n"
     ]
    }
   ],
   "source": [
    "### Parametrisation\n",
    "\n",
    "model = VGGLikeNN([(1, 32), (32, 128), (128, 256), (256, 512)], 63)\n",
    "optimizer = optim.Adam(model.parameters(), lr=5e-3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "writer = SummaryWriter(\"runsFineTune/vgg_training6\")\n",
    "epochs = 30\n",
    "\n",
    "model = train_model(model, train_loader, val_loader, epochs, criterion, optimizer, writer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f74f8a47",
   "metadata": {},
   "source": [
    "#### **Model 7**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5a3b35b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/30 | Train Loss: 0.4537, Train Acc: 0.8309 | Val Loss: 0.4060, Val Acc: 0.8429\n",
      "Epoch 20/30 | Train Loss: 0.3709, Train Acc: 0.8542 | Val Loss: 0.3353, Val Acc: 0.8714\n",
      "Epoch 30/30 | Train Loss: 0.3244, Train Acc: 0.8710 | Val Loss: 0.3067, Val Acc: 0.8814\n"
     ]
    }
   ],
   "source": [
    "### Parametrisation\n",
    "\n",
    "model = VGGLikeNN([(1, 32), (32, 256), (256, 512), (512, 1024)], 63)\n",
    "optimizer = optim.Adam(model.parameters(), lr=5e-3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "writer = SummaryWriter(\"runsFineTune/vgg_training7\")\n",
    "epochs = 30\n",
    "\n",
    "model = train_model(model, train_loader, val_loader, epochs, criterion, optimizer, writer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e922b1d2",
   "metadata": {},
   "source": [
    "#### **Performance evaluation**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4d1f3224",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"fine_tuned_vgg.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "744d213a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_ = VGGLikeNN([(1, 32), (32, 256), (256, 512), (512, 1024)], 63)\n",
    "model_.load_state_dict(torch.load(\"research/models/fine_tuned_vgg.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e6515c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, data_loader, criterion):\n",
    "    model.eval()\n",
    "    test_loss, corr, total = 0.0, 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in data_loader:\n",
    "\n",
    "            result = model(inputs)\n",
    "            outputs = result[0] if isinstance(result, tuple) else result\n",
    "\n",
    "            loss = criterion(outputs, labels)\n",
    "            test_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "            preds = outputs.argmax(dim=1)\n",
    "            corrects = (preds == labels)\n",
    "            corr += corrects.sum().item()\n",
    "            total += inputs.size(0)\n",
    "\n",
    "    avg_loss = test_loss / total\n",
    "    accuracy = corr / total\n",
    "\n",
    "    msg = f\"Loss: {avg_loss:.4f}, Acc: {accuracy:.4f}\"\n",
    "    print(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "49c59b97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.3294, Acc: 0.8744\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "evaluate_model(model_, test_loader, criterion)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
